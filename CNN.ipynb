{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./dataset/images\"\n",
    "\n",
    "image_names = os.listdir(image_dir)\n",
    "images = [os.path.join(image_dir, name) for name in image_names]\n",
    "\n",
    "#for i in images[:100]:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2d/kernel:0 is illegal; using conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d/bias:0 is illegal; using conv2d/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/kernel:0 is illegal; using conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_1/bias:0 is illegal; using conv2d_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/kernel:0 is illegal; using conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2d_2/bias:0 is illegal; using conv2d_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n"
     ]
    }
   ],
   "source": [
    "image_name_queue = tf.train.string_input_producer(images, shuffle=False)\n",
    "label_name_queue = tf.train.string_input_producer(['./dataset/label/Label.csv'], shuffle=False)\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "key, raw_image = image_reader.read(image_name_queue) #key는 filename, raw_image는 실제 image data(2진 데이터임)\n",
    "\n",
    "csv_reader = tf.TextLineReader()\n",
    "key, raw_txt = csv_reader.read(label_name_queue)\n",
    "\n",
    "png_image = tf.image.decode_png(raw_image) #컬러 이미지는 r, g, b로 3차원 Matix 형성\n",
    "csv_label = tf.decode_csv(raw_txt, record_defaults=[[0]])\n",
    "\n",
    "png_image = tf.reduce_mean(png_image, axis=2) #실습에서는 흑백 이미지 이기 때문에 차원을 2차로 축소\n",
    "png_image = tf.reshape(png_image, [61, 49, 1]) #image size는 높이 * 넓이, 1은 3차원을 만들기 위해 강제로 넣은 값\n",
    "\n",
    "#x, y_ = tf.train.batch([png_image, csv_label], 32) #32는 mini batch size\n",
    "x, y_ = tf.train.shuffle_batch([png_image, csv_label], batch_size=32, capacity=50000, min_after_dequeue=10000)\n",
    "#순차적으로 들어간 값(label)을 가져와서 Training 하면 일부 데이터로 훈련하기 때문에 suffle을 해야 함\n",
    "\n",
    "#print(x.shape)\n",
    "\n",
    "x = tf.cast(x, tf.float32) #x의 값이 integer여서 연산을 위해 float32로 변환\n",
    "\n",
    "#print(y_.shape)\n",
    "y_ = tf.reshape(y_, [-1])\n",
    "#print(y_.shape)\n",
    "\n",
    "y_ = tf.one_hot(y_, depth=3, on_value=1.0, off_value=0.0, axis=-1, dtype=tf.float32) #이해가 안됨\n",
    "#y_ = tf.reduce_mean(y_, axis=2)\n",
    "\n",
    "conv1 = tf.layers.conv2d(x, filters=13, kernel_size=[3, 3], activation=tf.nn.relu, padding=\"SAME\") #일반적으로 kernel_size는 3*3을 선호한다.\n",
    "pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=[2, 2]) #pool_size를 [2, 2]로 써도 무방함\n",
    "\n",
    "#print(pool1.shape)\n",
    "tf.summary.image('feature_map', tf.reduce_mean(pool1, axis=3, keep_dims=True))\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, filters=20, kernel_size=[3, 3], activation=tf.nn.relu, padding=\"SAME\") #kernel_size는 3을 써도 무방\n",
    "pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=[2, 2]) \n",
    "\n",
    "conv3 = tf.layers.conv2d(pool2, filters=30, kernel_size=[3, 3], activation=tf.nn.relu, padding=\"SAME\") #일반적으로 kernel_size는 3*3을 선호한다.\n",
    "pool3 = tf.layers.max_pooling2d(conv3, pool_size=2, strides=[2, 2]) #pool_size를 [2, 2]로 써도 무방함\n",
    "\n",
    "fc_input_size = int(pool3.shape[1]) * int(pool3.shape[2]) * int(pool3.shape[3]) #이해가 안됨\n",
    "flat = tf.reshape(pool3, shape=[-1, fc_input_size]) #-1을 넣은 이유는 3차원 큐브가 batch로 인해 32개가 존재하는데\n",
    "                                                    #이것까지 input으로 넣으면 안되기 때문임\n",
    "\n",
    "drop_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "fc1 = tf.layers.dense(flat, units=1000)\n",
    "drop1 = tf.layers.dropout(fc1, drop_prob)\n",
    "\n",
    "fc2 = tf.layers.dense(drop1, units=500)\n",
    "drop2 = tf.layers.dropout(fc2, drop_prob)\n",
    "\n",
    "out = tf.layers.dense(fc2, units=3)\n",
    "\n",
    "for i, var in enumerate(tf.trainable_variables()):\n",
    "    #tf.summary.histogram('variable_{}'.format(i), var)\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(y_, out)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-6).minimize(loss)\n",
    "\n",
    "pred = tf.nn.softmax(out)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(y_, axis=1), tf.argmax(pred, axis=1))\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "loss: 6.185981750488281\n",
      "accuracy: (0.0, 0.4375)\n",
      "step: 100\n",
      "loss: 3.403373956680298\n",
      "accuracy: (0.5753125, 0.57425743)\n",
      "step: 200\n",
      "loss: 1.863694190979004\n",
      "accuracy: (0.52109373, 0.52052242)\n",
      "step: 300\n",
      "loss: 2.5606775283813477\n",
      "accuracy: (0.50156248, 0.50134969)\n",
      "step: 400\n",
      "loss: 2.368386745452881\n",
      "accuracy: (0.49164063, 0.49134976)\n",
      "step: 500\n",
      "loss: 3.16355299949646\n",
      "accuracy: (0.48725, 0.48708832)\n",
      "step: 600\n",
      "loss: 2.5184879302978516\n",
      "accuracy: (0.4819271, 0.48180115)\n",
      "step: 700\n",
      "loss: 2.068319797515869\n",
      "accuracy: (0.47897321, 0.47909236)\n",
      "step: 800\n",
      "loss: 1.756575345993042\n",
      "accuracy: (0.47683594, 0.47682583)\n",
      "step: 900\n",
      "loss: 2.150402784347534\n",
      "accuracy: (0.47427082, 0.4742994)\n",
      "step: 1000\n",
      "loss: 2.7075233459472656\n",
      "accuracy: (0.47256249, 0.47262114)\n",
      "step: 1100\n",
      "loss: 2.7922558784484863\n",
      "accuracy: (0.4728125, 0.47280881)\n",
      "step: 1200\n",
      "loss: 2.8890135288238525\n",
      "accuracy: (0.47252604, 0.47241881)\n",
      "step: 1300\n",
      "loss: 3.0305368900299072\n",
      "accuracy: (0.4725, 0.47256917)\n",
      "step: 1400\n",
      "loss: 1.316534399986267\n",
      "accuracy: (0.47234374, 0.47231889)\n",
      "step: 1500\n",
      "loss: 2.821228504180908\n",
      "accuracy: (0.471625, 0.4715398)\n",
      "step: 1600\n",
      "loss: 3.052912712097168\n",
      "accuracy: (0.47146484, 0.47144362)\n",
      "step: 1700\n",
      "loss: 3.1148366928100586\n",
      "accuracy: (0.47187501, 0.47183642)\n",
      "step: 1800\n",
      "loss: 2.6101322174072266\n",
      "accuracy: (0.47237846, 0.47251526)\n",
      "step: 1900\n",
      "loss: 2.3752832412719727\n",
      "accuracy: (0.47190788, 0.47193912)\n",
      "step: 2000\n",
      "loss: 2.5968594551086426\n",
      "accuracy: (0.47125, 0.47126436)\n",
      "step: 2100\n",
      "loss: 3.066007137298584\n",
      "accuracy: (0.47144344, 0.47150168)\n",
      "step: 2200\n",
      "loss: 2.170058488845825\n",
      "accuracy: (0.47079545, 0.47083712)\n",
      "step: 2300\n",
      "loss: 1.8533247709274292\n",
      "accuracy: (0.47103262, 0.47103161)\n",
      "step: 2400\n",
      "loss: 1.7836945056915283\n",
      "accuracy: (0.47054687, 0.47055915)\n",
      "step: 2500\n",
      "loss: 2.847960948944092\n",
      "accuracy: (0.47071251, 0.47072423)\n",
      "step: 2600\n",
      "loss: 2.13836669921875\n",
      "accuracy: (0.47078124, 0.47079247)\n",
      "step: 2700\n",
      "loss: 2.228189468383789\n",
      "accuracy: (0.47111112, 0.47107553)\n",
      "step: 2800\n",
      "loss: 2.085728645324707\n",
      "accuracy: (0.4714509, 0.47144994)\n",
      "step: 2900\n",
      "loss: 2.4739773273468018\n",
      "accuracy: (0.47182113, 0.4718416)\n",
      "step: 3000\n",
      "loss: 3.2209665775299072\n",
      "accuracy: (0.47237501, 0.47237378)\n",
      "step: 3100\n",
      "loss: 1.6942416429519653\n",
      "accuracy: (0.47255039, 0.47257942)\n",
      "step: 3200\n",
      "loss: 1.8293890953063965\n",
      "accuracy: (0.47253907, 0.47248906)\n",
      "step: 3300\n",
      "loss: 1.5161492824554443\n",
      "accuracy: (0.47289774, 0.47290593)\n",
      "step: 3400\n",
      "loss: 2.2530455589294434\n",
      "accuracy: (0.47307906, 0.47311452)\n",
      "step: 3500\n",
      "loss: 1.7649849653244019\n",
      "accuracy: (0.47318751, 0.47318622)\n",
      "step: 3600\n",
      "loss: 1.674241542816162\n",
      "accuracy: (0.47390625, 0.4738701)\n",
      "step: 3700\n",
      "loss: 2.184680461883545\n",
      "accuracy: (0.47434965, 0.47434816)\n",
      "step: 3800\n",
      "loss: 1.8460583686828613\n",
      "accuracy: (0.47446546, 0.47447217)\n",
      "step: 3900\n",
      "loss: 1.0091526508331299\n",
      "accuracy: (0.47510415, 0.47511855)\n",
      "step: 4000\n",
      "loss: 1.4876601696014404\n",
      "accuracy: (0.4754453, 0.47547489)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator() #Coordinator는 thread를 관리하는 역할을 함\n",
    "    thread = tf.train.start_queue_runners(sess, coord)\n",
    "    writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "    \n",
    "#    _image, _label = sess.run([x, y_])\n",
    "#    for i in range(32):\n",
    "#        plt.figure(i)\n",
    "#        print('age: {}'.format(_label[i]))\n",
    "#        plt.imshow(_image[i])\n",
    "\n",
    "#    print(y_)\n",
    "\n",
    "    for i in range(10000):\n",
    "        _, _loss, _summaries = sess.run([train_op, loss, merged], feed_dict={drop_prob: 0.7})\n",
    "        _pred, _accuracy = sess.run([pred, accuracy], feed_dict={drop_prob: 1.0})\n",
    "        writer.add_summary(_summaries, i)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('step: {}'.format(i))\n",
    "            print('loss: {}'.format(_loss))\n",
    "            print('accuracy: {}'.format(_accuracy))\n",
    "            #print('prediction 1: {}'.format(tf.argmax(_pred[0])))\n",
    "            #print('prediction 2: {}'.format(tf.argmax(_pred[1])))\n",
    "            #print('prediction 3: {}'.format(tf.argmax(_pred[2])))\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
